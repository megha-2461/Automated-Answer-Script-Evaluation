# Automated Answer Script Evaluation System

## ğŸ“Œ Overview
The Automated Answer Script Evaluation System is a group project aimed at reducing
manual effort in checking answer scripts. The system converts student answer scripts
into digital text and evaluates them automatically by comparing them with model
answers using NLP-based techniques.

This project demonstrates the use of OCR, text preprocessing, and similarity-based
scoring to assign marks in an efficient and transparent manner.

---

## ğŸ¯ Objectives
- Digitize handwritten or typed answer scripts
- Automatically evaluate answers using NLP techniques
- Assign marks based on similarity with model answers
- Reduce human effort and evaluation time
- Provide consistent and unbiased assessment

---

## ğŸ› ï¸ Features
- OCR-based text extraction from answer scripts
- Text preprocessing (cleaning, normalization)
- Automated answer comparison
- Similarity-based scoring
- Simple and interpretable marking logic
- User-friendly interface for evaluation results

---

## ğŸ§  System Workflow
1. Upload answer script (image/PDF/text)
2. Extract text using OCR
3. Preprocess extracted text
4. Compare with model answers
5. Calculate similarity score
6. Generate marks and feedback

---

## ğŸ’» Tech Stack
- **Programming Language:** Python
- **OCR:** Tesseract OCR
- **NLP:** TF-IDF, Cosine Similarity
- **Backend / UI:** Flask or Streamlit
- **Version Control:** Git & GitHub

---

## ğŸ‘¥ Team Members
- Megha Goswami
- Romil Raj
- Muskan Goyal

